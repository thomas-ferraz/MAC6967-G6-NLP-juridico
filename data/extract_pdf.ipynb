{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition from .pdf files\n",
    "\n",
    "This notebook acquires data from the dataset2/falencias\n",
    "\n",
    "Requirements:\n",
    "\n",
    "*   textract - Enables easier extraction of text from .pdf files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells are needed to run the notebook inside the Google Colaboratory platform, with the datasets in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3234,
     "status": "ok",
     "timestamp": 1600386299849,
     "user": {
      "displayName": "Pedro Henrique Barbosa de Almeida",
      "photoUrl": "",
      "userId": "11730038738616894500"
     },
     "user_tz": 180
    },
    "id": "Fpa6xAMVfCoq",
    "outputId": "4158c01e-ecca-4454-f9e7-e25a9889dab9"
   },
   "outputs": [],
   "source": [
    "!pip install textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up connection to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 765,
     "status": "ok",
     "timestamp": 1600386301232,
     "user": {
      "displayName": "Pedro Henrique Barbosa de Almeida",
      "photoUrl": "",
      "userId": "11730038738616894500"
     },
     "user_tz": 180
    },
    "id": "m_9pSOhzfG-t",
    "outputId": "1aadf26e-ce15-4c42-eb44-b7794bd0e3cf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# dataset2 location in Google Drive\n",
    "PATH_falencias = '/content/drive/Shared drives/NLP Jurídico - Lab. Ciência de Dados/Dados PDFs falencias'\n",
    "PATH_save = '/content/drive/Shared drives/NLP Jurídico - Lab. Ciência de Dados'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is needed to run the notebook locally, with the datasets available in a local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_falencias = '/run/media/raktanaka/KINGSTON/dataset3'\n",
    "PATH_save = '/run/media/raktanaka/KINGSTON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import textract\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_data(suit, path_pdf, log_succ, log_fail):\n",
    "\n",
    "    full_file_name = os.path.split(path_pdf)[1]\n",
    "    row_dict = {}\n",
    "\n",
    "    try:\n",
    "        begin_parenthesis_i = full_file_name.find('(')\n",
    "        doc_type = full_file_name[:begin_parenthesis_i-1]\n",
    "        # Normalizes graphic accentuation\n",
    "        begin_sheet_no = 0\n",
    "        end_sheet_no = 0\n",
    "        signer_name = ''\n",
    "\n",
    "        pag_found = False\n",
    "        file_name = full_file_name[begin_parenthesis_i:]\n",
    "\n",
    "        '''\n",
    "        Gets the initial and final page from the file name\n",
    "        '''\n",
    "        # Tries to get what's between two parenthesis (...). If the string \"pag\"\n",
    "        # isn't present, tries to get two more parenthesis (...) in the file name.\n",
    "        # This is crucial because there are filenames like \"Bla (AR) (pag. xxx - xxx).pdf\"\n",
    "\n",
    "        while(not pag_found):\n",
    "            begin_parenthesis_i = file_name.find('(')\n",
    "            end_parenthesis_i = file_name.find(')')\n",
    "\n",
    "            # Checks if there're still substrings with \"(...)\"\n",
    "            if begin_parenthesis_i != -1:\n",
    "                # Checks if there's something like \"(pag)\" within the substring\n",
    "                if 'pag' in file_name[begin_parenthesis_i:end_parenthesis_i]:\n",
    "                    pag_found = True\n",
    "                    # Checks if there're one or two pages in the file name\n",
    "                    dash_index = file_name.find('-')\n",
    "                    if dash_index != -1:\n",
    "                        begin_sheet_no = int(\n",
    "                            file_name[begin_parenthesis_i+4:dash_index-1])\n",
    "                        end_sheet_no = int(\n",
    "                            file_name[dash_index+1:end_parenthesis_i])\n",
    "                    else:\n",
    "                        begin_sheet_no = int(\n",
    "                            file_name[begin_parenthesis_i+4:end_parenthesis_i])\n",
    "                        end_sheet_no = int(\n",
    "                            file_name[begin_parenthesis_i+4:end_parenthesis_i])\n",
    "            else:\n",
    "                pag_found = True\n",
    "            file_name = file_name[end_parenthesis_i+1:]\n",
    "\n",
    "        '''\n",
    "        Extracts the string from the PDF\n",
    "        '''\n",
    "        if (len(full_file_name) > 4 and full_file_name[-4:] == '.pdf'):\n",
    "            text = textract.process(path_pdf)\n",
    "\n",
    "        row_dict = {'n_processo': suit, 'tipo_documento': doc_type,\n",
    "                    'string': text, 'n_folha_inicio': begin_sheet_no, \n",
    "                    'n_folha_fim': end_sheet_no}\n",
    "\n",
    "        log_succ.write(suit + ' ' + full_file_name + '\\n')\n",
    "\n",
    "    except:\n",
    "        log_fail.write(suit + ' ' + full_file_name + '\\n')\n",
    "        pass\n",
    "\n",
    "    return(row_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_dirs(root, dirs):\n",
    "\n",
    "    path = os.path.join(root, dirs)\n",
    "    subroot, subdirs, files = next(os.walk(path))\n",
    "    \n",
    "    if subdirs:\n",
    "        for each_dir in subdirs:\n",
    "            subfiles = walk_dirs(subroot, each_dir)\n",
    "            subfiles = [os.path.join(each_dir, f) for f in subfiles]\n",
    "            files.extend(subfiles)\n",
    "\n",
    "    return(files)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural sort, so the pdf files are ordered alphanumerically:\n",
    "# Documento1, Documento2, Documento10\n",
    "# And not\n",
    "# Documento1, Documento10, Documento2\n",
    "def NaturalSort(l):\n",
    "\n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Files: success and failure logs, and the csv with pdf data\n",
    "log_succ = open(os.path.join(PATH_save, 'pdf_success.log'), 'w')\n",
    "log_fail = open(os.path.join(PATH_save, 'pdf_error.log'), 'w')\n",
    "csv_pdf = open(os.path.join(PATH_save, 'csv_pdf.csv'), 'w', newline='')\n",
    "\n",
    "csv_columns = ['n_processo', 'tipo_documento', 'string', 'n_folha_inicio', 'n_folha_fim']\n",
    "csv_writer = csv.DictWriter(csv_pdf, fieldnames=csv_columns)\n",
    "csv_writer.writeheader()\n",
    "\n",
    "# Get all processes = directories\n",
    "root, dirs, files = next(os.walk(PATH_falencias))\n",
    "dirs = NaturalSort(dirs)\n",
    "\n",
    "for suit in dirs:\n",
    "    p_root, p_dirs, p_files = next(os.walk(os.path.join(root, suit)))\n",
    "    # If has subdirectories, gets the pdfs inside\n",
    "    if p_dirs:\n",
    "        p_dirs = NaturalSort(p_dirs)\n",
    "        for each_dir in p_dirs:\n",
    "            # Skips hidden or system directories\n",
    "            if not each_dir.startswith('.') and not each_dir.startswith('System'):\n",
    "                subfiles = walk_dirs(p_root, each_dir)\n",
    "                subfiles = [os.path.join(each_dir, f) for f in subfiles]\n",
    "                p_files.extend(subfiles)\n",
    "\n",
    "    # With the list of all pdfs, starts processing the data\n",
    "    p_files = NaturalSort(p_files)\n",
    "    for pdf in p_files:\n",
    "        # Skips hidden or system files\n",
    "        if not pdf.startswith('.') and not pdf.startswith('System'):\n",
    "            row_dict = get_pdf_data(suit, os.path.join(p_root, pdf), log_succ, log_fail)\n",
    "            if row_dict:\n",
    "                csv_writer.writerow(row_dict)\n",
    "\n",
    "log_succ.close()\n",
    "log_fail.close()\n",
    "csv_pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Abrir PDFs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('3.8.5')",
   "language": "python",
   "name": "python_defaultSpec_1602106627917"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}