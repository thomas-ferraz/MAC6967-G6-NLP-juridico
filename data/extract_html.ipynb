{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DadosJuridicos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "source": [
        "# Data acquisition from .html files\n",
        "\n",
        "This notebook acquires data from the dataset1\n",
        "\n",
        "Requirements:\n",
        "\n",
        "*   dateparser - Enables parsing of dates."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "The next cells are needed to run the notebook inside the Google Colaboratory platform, with the datasets in Google Drive"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T81HLreyaBrs",
        "outputId": "549a664e-877e-4f2e-f9aa-0bf21878187c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# dataset1 location in Google Drive\n",
        "PATH_falencias = '/content/drive/My Drive/6-Processamento de Linguagem Natural em decisões judiciais/dataset1/falencias'\n",
        "PATH_recjud = '/content/drive/My Drive/6-Processamento de Linguagem Natural em decisões judiciais/dataset1/recuperacoes_judiciais'\n",
        "PATH_save = '/content/drive/Shared drives/NLP Jurídico - Lab. Ciência de Dados'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMPEDJiZkUfP",
        "outputId": "641b593d-b92c-4dc7-9610-37f8936bd094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "!pip install dateparser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6z6KpCxfIxe"
      },
      "source": [
        "#Import packages\n",
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import dateparser\n",
        "from dateparser.search import search_dates\n",
        "#from sklearn.manifold import TSNE\n",
        "#import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "import time\n",
        "from time import sleep\n",
        "import unidecode\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import spacy\n",
        "#Libraries and functions\n",
        "#import utils\n",
        "#from utils import *\n",
        "from dateutil.parser import parse\n",
        "#Functions\n",
        "from collections import Counter\n",
        "import string\n",
        "#from top2vec import Top2Vec\n",
        "#import gensim\n",
        "#import logging\n",
        "#import scipy.sparse\n",
        "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "#Libraries and functions\n",
        "#import utils\n",
        "#from utils import *\n",
        "#import pyreadr\n",
        "#import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import lxml\n",
        "from lxml.html.clean import Cleaner\n",
        "from lxml.html import fromstring"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3OXC1tWaJPH",
        "outputId": "b3e566e7-b59b-4484-d05a-b00430c2a924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Get data from drive\n",
        "dataset2f = '/content/drive/Shared drives/NLP Jurídico - Lab. Ciência de Dados/falencias'\n",
        "\n",
        "root2f, dirs2f, files2f = next(os.walk(dataset2f))\n",
        "nfiles2f = len(dirs2f)\n",
        "\n",
        "nfiles2f = 0\n",
        "size2f = 0\n",
        "for each_dir in dirs2f:\n",
        "  sub_path = os.path.join(root2f, each_dir)\n",
        "  sub_root, sub_dirs, sub_files = next(os.walk(sub_path))\n",
        "  nfiles2f = nfiles2f + len(sub_files)\n",
        "  for each_file in sub_files:\n",
        "    size2f = size2f + os.path.getsize(os.path.join(sub_root, each_file))\n",
        "\n",
        "\"\"\"Coleta de dados dos processos\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZwfRl3GameI"
      },
      "source": [
        "#Get data from csv uploaded from drive\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "dataset2f = '/content/drive/Shared drives/NLP Jurídico - Lab. Ciência de Dados/falencias.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "168XdGx1aIWD",
        "outputId": "819f44e1-6f7b-4707-e7ba-01311db60649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#Main data\n",
        "df = pd.read_csv(dataset2f)\n",
        "df['NumeroDoProcesso']=df['NumeroDoProcesso'].str.replace(']','').str.replace('[','')\n",
        "df['html']=df['html'].str.replace(']','').str.replace('[','')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvz5ffD-ezBz"
      },
      "source": [
        "#Cleaning\n",
        "for i in range(0,len(df)):\n",
        "    \n",
        "    soup = BeautifulSoup(df['html'][i])\n",
        "\n",
        "    # kill all script and style elements\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()    # rip it out\n",
        "\n",
        "    # get text\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    # break multi-headlines into a line each\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    # drop blank lines\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "    text = \"\".join(text.split('\\\\n'))\n",
        "    text = \"\".join(text.split('\\\\t')).replace('\\\\n',' ')\n",
        "    #text = \"\".join(text.split('\\n'))\n",
        "    text = text.replace('\\\\xa0','')\n",
        "    text = text.replace('Advogada','Advogado')\n",
        "    df['html'][i] = text.lower()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUcCAGgXj2aZ"
      },
      "source": [
        "#New dataframe\n",
        "dfinal = pd.DataFrame(columns=['num_proc','status','juiz','terceiros','rep_legal','valor_acao','reqte','adv_reqte','reqdo','adv_reqdo'])\n",
        "errors=[]\n",
        "for i in range(0,len(df)):\n",
        "  #print(i)\n",
        "  try:\n",
        "  #Numero processo e satus\n",
        "    a=\"\".join(re.findall(\"[\\d.-]\", df['html'][i].replace('\\n','').split('dados do processo')[1].split('processo:')[1].split('classe:')[0]))\n",
        "    #s=\"\".join(re.findall(\"[^\\d\\W]\", df['html'][i].replace('\\n','').split('dados do processo')[1].split('processo:')[1].split('classe:')[0]))\n",
        "    s=\"\".join(re.findall(\"[^\\d:.,-]+\", df['html'][i].replace('\\n','').split('dados do processo')[1].split('processo:')[1].split('classe:')[0]))\n",
        "  except:\n",
        "    a=''\n",
        "    s=''\n",
        "    errors+=[i]\n",
        "\n",
        "  try:\n",
        "\n",
        "    t=df['html'][i].split('partes do processo')[1].split('movimentações')[0].split('terintcer:')[1].split('\\n')[0]\n",
        "    r=\",\".join(list(filter(lambda x: 'repreleg:' in x,df['html'][i].split('reqdo')[1].split('\\n'))))\n",
        "    r=r.replace('repreleg:','')\n",
        "  except:\n",
        "    t=r=''\n",
        "\n",
        "  try:\n",
        "    #Juiz, terceiros, representante\n",
        "    #b=df['html'][i].split('dados do processo')[1].split('juiz:')[1].split('valor da ação:')[0].replace('\\n','')\n",
        "    if 'outros' in df['html'][i].split('dados do processo')[1].split('juiz:')[1].split('valor da ação:')[0]:\n",
        "      b=\" \".join(re.findall(r'[^\\s0-9-,.]+',df['html'][i].split('dados do processo')[1].split('juiz:')[1].split('valor da ação:')[0])[:-2])\n",
        "    else:\n",
        "      b=\" \".join(re.findall(r'[^\\s0-9-,.]+',df['html'][i].split('dados do processo')[1].split('juiz:')[1].split('valor da ação:')[0]))\n",
        "\n",
        "  except:\n",
        "    b=''\n",
        "    errors+=[i]\n",
        "  #Valor da ação\n",
        "  try:\n",
        "    c=re.findall(\"r[$]\\d{1,3}.\\d{3}\\,\\d{2}\", df['html'][i].replace('\\n','').split('dados do processo')[1].split('processo:')[1].split('valor da ação:')[1])[0]\n",
        "    c=c.replace('r$','')\n",
        "    #c=re.findall(\"[r$]\\d{1,3}.\\d{3}\\,\\d{2}\", df['html'][i].replace('\\n','').split('dados do processo')[1].split('processo:')[1].split('valor da ação:')[1])[0]\n",
        "  except:\n",
        "    c=''\n",
        "    errors+=[i]\n",
        "  try:\n",
        "    if 'reqdo' in df['html'][i]:\n",
        "      x='reqdo'\n",
        "    else:\n",
        "      x='falido'\n",
        "    #Requerente\n",
        "    d=\" \".join(re.findall(r'[^\\s0-9-,.]+',df['html'][i].split('reqte:')[1].split('advogado')[0]))\n",
        "    #d=df['html'][i].split('reqte:')[1].split('advogado')[0]\n",
        "    #Advogado Requerente\n",
        "    e=\" \".join(re.findall(r'[^\\s0-9-,.:]+',df['html'][i].split('reqte:')[1].split('advogado')[1].split(x)[0]))\n",
        "    #e=df['html'][i].split('reqte:')[1].split('advogado')[1].split(x)[0]\n",
        "    #Requerido\n",
        "    #f=\" \".join(re.findall(r'[^\\s0-9-,.:]+',df['html'][i].split(x)[1].split('advogado')[0]))\n",
        "    #f=df['html'][i].split(x)[1].split('advogado')[0]\n",
        "    f=re.findall(r'[^0-9-,.:]+',df['html'][i].split(x)[1].split('\\n')[0])[0]\n",
        "    #Advogado Requerido\n",
        "    g=\",\".join(list(filter(lambda x: 'advogado:' in x,df['html'][i].split(x)[1].split('\\n'))))\n",
        "    g=g.replace('advogado:','')\n",
        "   \n",
        "  except:\n",
        "    errors+=[i]\n",
        "    d=e=f=g=''\n",
        "\n",
        "  dfinal=dfinal.append({'num_proc':a,'status':s,'juiz':b,'terceiros':t,'rep_legal':r,'valor_acao':c,'reqte':d,'adv_reqte':e,'reqdo':f,'adv_reqdo':g},ignore_index=True)\n",
        "  errors=list(set(errors))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAsop_xUEihH",
        "outputId": "4c90fb99-ae3d-4f09-c7a2-d831d7a5f07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        }
      },
      "source": [
        "dfinal"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}