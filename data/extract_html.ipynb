{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DadosJuridicos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python_defaultSpec_1601747266094",
      "display_name": "Python 3.8.5 64-bit ('3.8.5')",
      "metadata": {
        "interpreter": {
          "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
        }
      }
    }
  },
  "cells": [
    {
      "source": [
        "# Data acquisition from .html files\n",
        "\n",
        "This notebook acquires data from the dataset1\n",
        "\n",
        "Requirements:\n",
        "\n",
        "*   Beautiful Soup - Removes html tags.\n",
        "*   Pandas - Data manipulation."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "The next cells are needed to run the notebook inside the Google Colaboratory platform, with the datasets in Google Drive"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install dateparser"
      ]
    },
    {
      "source": [
        "Setting up connection to Google Drive"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T81HLreyaBrs",
        "outputId": "549a664e-877e-4f2e-f9aa-0bf21878187c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# dataset1 location in Google Drive\n",
        "CSV_falencias = '/content/drive/Shared drives/NLP Jurídico - Lab. Ciência de Dados/falencias.csv'\n",
        "CSV_recjud = '/content/drive/Shared drives/NLP Jurídico - Lab. Ciência de Dados/recuperacoes_judiciais.csv'\n",
        "PATH_save = '/content/drive/Shared drives/NLP Jurídico - Lab. Ciência de Dados'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "The next cell is needed to run the notebook locally, with the datasets available in a local disk"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CSV_falencias = '/home/raktanaka/USP/dataset/dataset1/falencias.csv'\n",
        "#CSV_unzip = '/home/raktanaka/USP/dataset/dataset1/recuperacoes_judiciais.csv'\n",
        "CSV_falencias = '/run/media/raktanaka/KINGSTON/dataset1/falencias.csv'\n",
        "CSV_recjud = '/run/media/raktanaka/KINGSTON/dataset1/recuperacoes_judiciais.csv'\n",
        "PATH_save = '/home/raktanaka/USP/MAC6967-G6-NLP-juridico'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "9223372036854775807"
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Increase CSV allocation size\n",
        "csv.field_size_limit(sys.maxsize)"
      ]
    },
    {
      "source": [
        "2 Functions for data extraction"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvz5ffD-ezBz"
      },
      "source": [
        "#Cleaning\n",
        "def Cleaning(df):\n",
        "\n",
        "    for i in range(0,len(df)):\n",
        "        \n",
        "        soup = BeautifulSoup(df['html'][i])\n",
        "\n",
        "        # kill all script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.extract()    # rip it out\n",
        "\n",
        "        # get text\n",
        "        text = soup.get_text()\n",
        "\n",
        "        # break into lines and remove leading and trailing space on each\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        text = \"\".join(text.split('\\\\n'))\n",
        "        text = \"\".join(text.split('\\\\t')).replace('\\\\n',' ')\n",
        "        #text = \"\".join(text.split('\\n'))\n",
        "        text = text.replace('\\\\xa0','')\n",
        "        text = text.replace('Advogada','Advogado')\n",
        "        df['html'][i] = text.lower()\n",
        "\n",
        "    return(df)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUcCAGgXj2aZ"
      },
      "source": [
        "def GetData(df):\n",
        "\n",
        "  #New dataframe\n",
        "  dfinal = pd.DataFrame(columns=['num_proc','status','juiz','terceiros','rep_legal','valor_acao','reqte','adv_reqte','reqdo','adv_reqdo'])\n",
        "  errors=[]\n",
        "  for i in range(0,len(df)):\n",
        "    #print(i)\n",
        "    try:\n",
        "    #Numero processo e satus\n",
        "      a=\"\".join(re.findall(\"[\\d.-]\", df['html'][i].replace('\\n','').split('dados do processo')[1].split('processo:')[1].split('classe:')[0]))\n",
        "      #s=\"\".join(re.findall(\"[^\\d\\W]\", df['html'][i].replace('\\n','').split('dados do processo')[1].split('processo:')[1].split('classe:')[0]))\n",
        "      s=\"\".join(re.findall(\"[^\\d:.,-]+\", df['html'][i].replace('\\n','').split('dados do processo')[1].split('processo:')[1].split('classe:')[0]))\n",
        "    except:\n",
        "      a=''\n",
        "      s=''\n",
        "      errors+=[i]\n",
        "\n",
        "    try:\n",
        "\n",
        "      t=df['html'][i].split('partes do processo')[1].split('movimentações')[0].split('terintcer:')[1].split('\\n')[0]\n",
        "      r=\",\".join(list(filter(lambda x: 'repreleg:' in x,df['html'][i].split('reqdo')[1].split('\\n'))))\n",
        "      r=r.replace('repreleg:','')\n",
        "    except:\n",
        "      t=r=''\n",
        "\n",
        "    try:\n",
        "      #Juiz, terceiros, representante\n",
        "      #b=df['html'][i].split('dados do processo')[1].split('juiz:')[1].split('valor da ação:')[0].replace('\\n','')\n",
        "      if 'outros' in df['html'][i].split('dados do processo')[1].split('juiz:')[1].split('valor da ação:')[0]:\n",
        "        b=\" \".join(re.findall(r'[^\\s0-9-,.]+',df['html'][i].split('dados do processo')[1].split('juiz:')[1].split('valor da ação:')[0])[:-2])\n",
        "      else:\n",
        "        b=\" \".join(re.findall(r'[^\\s0-9-,.]+',df['html'][i].split('dados do processo')[1].split('juiz:')[1].split('valor da ação:')[0]))\n",
        "\n",
        "    except:\n",
        "      b=''\n",
        "      errors+=[i]\n",
        "    #Valor da ação\n",
        "    try:\n",
        "      c=re.findall(\"r[$]\\d{1,3}.\\d{3}\\,\\d{2}\", df['html'][i].replace('\\n','').split('dados do processo')[1].split('processo:')[1].split('valor da ação:')[1])[0]\n",
        "      c=c.replace('r$','')\n",
        "      #c=re.findall(\"[r$]\\d{1,3}.\\d{3}\\,\\d{2}\", df['html'][i].replace('\\n','').split('dados do processo')[1].split('processo:')[1].split('valor da ação:')[1])[0]\n",
        "    except:\n",
        "      c=''\n",
        "      errors+=[i]\n",
        "    try:\n",
        "      if 'reqdo' in df['html'][i]:\n",
        "        x='reqdo'\n",
        "      else:\n",
        "        x='falido'\n",
        "      #Requerente\n",
        "      d=\" \".join(re.findall(r'[^\\s0-9-,.]+',df['html'][i].split('reqte:')[1].split('advogado')[0]))\n",
        "      #d=df['html'][i].split('reqte:')[1].split('advogado')[0]\n",
        "      #Advogado Requerente\n",
        "      e=\" \".join(re.findall(r'[^\\s0-9-,.:]+',df['html'][i].split('reqte:')[1].split('advogado')[1].split(x)[0]))\n",
        "      #e=df['html'][i].split('reqte:')[1].split('advogado')[1].split(x)[0]\n",
        "      #Requerido\n",
        "      #f=\" \".join(re.findall(r'[^\\s0-9-,.:]+',df['html'][i].split(x)[1].split('advogado')[0]))\n",
        "      #f=df['html'][i].split(x)[1].split('advogado')[0]\n",
        "      f=re.findall(r'[^0-9-,.:]+',df['html'][i].split(x)[1].split('\\n')[0])[0]\n",
        "      #Advogado Requerido\n",
        "      g=\",\".join(list(filter(lambda x: 'advogado:' in x,df['html'][i].split(x)[1].split('\\n'))))\n",
        "      g=g.replace('advogado:','')\n",
        "    \n",
        "    except:\n",
        "      errors+=[i]\n",
        "      d=e=f=g=''\n",
        "\n",
        "    dfinal=dfinal.append({'num_proc':a,'status':s,'juiz':b,'terceiros':t,'rep_legal':r,'valor_acao':c,'reqte':d,'adv_reqte':e,'reqdo':f,'adv_reqdo':g},ignore_index=True)\n",
        "    errors=list(set(errors))\n",
        "  return(dfinal)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "source": [
        "The next 2 cells execute the data extraction"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Main data - falencias\n",
        "DF_save = 'falencias_html.csv'\n",
        "\n",
        "df = pd.read_csv(CSV_falencias)\n",
        "df['NumeroDoProcesso']=df['NumeroDoProcesso'].str.replace(']','').str.replace('[','')\n",
        "df['html']=df['html'].str.replace(']','').str.replace('[','')\n",
        "\n",
        "df_falencias = Cleaning(df)\n",
        "df_falencias = GetData(df_falencias)\n",
        "\n",
        "save = os.path.join(PATH_save, DF_save)\n",
        "df_falencias.to_csv(save, index = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Main data - recuperacoes_judiciais\n",
        "DF_save = 'recuperacoes_judiciais_html.csv'\n",
        "\n",
        "df = pd.read_csv(CSV_recjud)\n",
        "df['NumeroDoProcesso']=df['NumeroDoProcesso'].str.replace(']','').str.replace('[','')\n",
        "df['html']=df['html'].str.replace(']','').str.replace('[','')\n",
        "\n",
        "df_recjud = Cleaning(df)\n",
        "df_recjud = GetData(df_recjud)\n",
        "\n",
        "save = os.path.join(PATH_save, DF_save)\n",
        "df_recjud.to_csv(save, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}